{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbacfc2-95a2-4809-a3f1-a8e73d3d467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\begin{figure}[htbp]\n",
    "  \\centering\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Twin_Bias.png}\n",
    "    \\caption{Bias}\n",
    "    \\label{fig:bias}\n",
    "  \\end{minipage}\n",
    "  \\hfill\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Twin_Darks.png}\n",
    "    \\caption{Darks}\n",
    "    \\label{fig:darks}\n",
    "  \\end{minipage}\n",
    "\\end{figure}\n",
    "\\vspace{-15pt}  \n",
    "\\begin{figure}[htbp]\n",
    "  \\centering\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Twin_Flats.png}\n",
    "    \\caption{Flats}\n",
    "    \\label{fig:bias}\n",
    "  \\end{minipage}\n",
    "  \\hfill\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Twin_reduced_science.png}\n",
    "    \\caption{PN-M 29 Nebula}\n",
    "    \\label{fig:darks}\n",
    "  \\end{minipage}\n",
    "\\end{figure}\n",
    "\\vspace{-15pt}  \n",
    "\\begin{figure}[htbp]\n",
    "  \\centering\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Eagle_Final.png}\n",
    "    \\caption{Eagle Nebula}\n",
    "    \\label{fig:bias}\n",
    "  \\end{minipage}\n",
    "  \\hfill\n",
    "  \\begin{minipage}[b]{0.45\\textwidth}\n",
    "    \\includegraphics[width=\\textwidth]{Images/Reduced_Science_Image.png}\n",
    "    \\caption{Ring Nebula}\n",
    "    \\label{fig:darks}\n",
    "  \\end{minipage}\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4223c3-c771-48cd-963d-4542e1005a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_aperture_photometry(positions,radii = [3, 4, 5],sky_radius_in = 6, sky_annulus_width = 3):\n",
    "\n",
    "\n",
    "    output_file = Path(\"Twin_Images\") / 'Twin_aperture_photometry.fits'\n",
    "    input_dir = Path(\"Twin_Images\")\n",
    "\n",
    "    image_path = input_dir / 'twin_science_reduced.fits'\n",
    "    image_data = fits.getdata(image_path).astype('f4')\n",
    "\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    #Doing a loop over the star\n",
    "    for p in positions:\n",
    "        #Creating dictionary with stars position\n",
    "        row = {'xcenter': p[0], 'ycenter': p[1]}\n",
    "\n",
    "        #Looping over the radii for each star\n",
    "        for r in radii:\n",
    "            \n",
    "            # Doing the aperture and annulus stuff for one star\n",
    "            aperture = CircularAperture(p, r=r)\n",
    "            annulus = CircularAnnulus(p, r_in=sky_radius_in, r_out=sky_radius_in + sky_annulus_width)\n",
    "\n",
    "            # Calculating the aperture and annulus photometry\n",
    "            phot_ap = aperture_photometry(image_data, aperture)\n",
    "            stats = ApertureStats(image_data, annulus, sigma_clip=None)\n",
    "\n",
    "            # finding area of circle and subtracting the sky background to find total flux\n",
    "            aperture_area = aperture.area\n",
    "            background = stats.median * aperture_area\n",
    "            flux = phot_ap['aperture_sum'][0] - background\n",
    "\n",
    "            #Storing results in dictionary with star position and flux \n",
    "            row[f'flux_r{int(r)}'] = flux\n",
    "\n",
    "        #Putting the data into emtpy list\n",
    "        rows.append(row)\n",
    "\n",
    "    # Creating astropy table from dictionary data and adding radii/sky as metadata stuff\n",
    "    table = Table(rows)\n",
    "    table.meta['radii'] = radii\n",
    "    table.meta['sky_radius'] = sky_radius_in\n",
    "\n",
    "    plt.imshow(image_data, cmap = 'inferno', origin = 'lower', aspect = 'auto')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(Path(\"Twin_Images\") / 'Twin_photometry.png')\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    table.write(output_file, format='fits', overwrite=True)\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2ced12c-136c-4558-82b4-a1f49a237a7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twin_science_reduced.fits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphotutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAOStarFinder\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m image_data = \u001b[43mfits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetdata\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtwin_science_reduced.fits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m table = Table(rows)  \u001b[38;5;66;03m# contains xcenter, ycenter, flux_r3, flux_r5, ...\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Set up figure with background science image\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/convenience.py:220\u001b[39m, in \u001b[36mgetdata\u001b[39m\u001b[34m(filename, header, lower, upper, view, *args, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m extver = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mextver\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    216\u001b[39m ext_given = \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mlen\u001b[39m(args) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m extname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m extver \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    218\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m hdulist, extidx = \u001b[43m_getext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    222\u001b[39m     hdu = hdulist[extidx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/convenience.py:1121\u001b[39m, in \u001b[36m_getext\u001b[39m\u001b[34m(filename, mode, ext, extname, extver, *args, **kwargs)\u001b[39m\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m extver \u001b[38;5;129;01mand\u001b[39;00m extname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mextver alone cannot specify an extension.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m hdulist = \u001b[43mfitsopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hdulist, ext\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/hdulist.py:224\u001b[39m, in \u001b[36mfitsopen\u001b[39m\u001b[34m(name, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, decompress_in_memory, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmpty filename: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHDUList\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/hdulist.py:488\u001b[39m, in \u001b[36mHDUList.fromfile\u001b[39m\u001b[34m(cls, fileobj, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfromfile\u001b[39m(\n\u001b[32m    471\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m     **kwargs,\n\u001b[32m    480\u001b[39m ):\n\u001b[32m    481\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    Creates an `HDUList` instance from a file-like object.\u001b[39;00m\n\u001b[32m    483\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    486\u001b[39m \u001b[33;03m    documentation for details of the parameters accepted by this method).\u001b[39;00m\n\u001b[32m    487\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readfrom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/hdu/hdulist.py:1189\u001b[39m, in \u001b[36mHDUList._readfrom\u001b[39m\u001b[34m(cls, fileobj, data, mode, memmap, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, decompress_in_memory, **kwargs)\u001b[39m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, _File):\n\u001b[32m   1188\u001b[39m         \u001b[38;5;66;03m# instantiate a FITS file object (ffo)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m         fileobj = \u001b[43m_File\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m     \u001b[38;5;66;03m# The Astropy mode is determined by the _File initializer if the\u001b[39;00m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;66;03m# supplied mode was None\u001b[39;00m\n\u001b[32m   1200\u001b[39m     mode = fileobj.mode\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/file.py:218\u001b[39m, in \u001b[36m_File.__init__\u001b[39m\u001b[34m(self, fileobj, mode, memmap, overwrite, cache, use_fsspec, fsspec_kwargs, decompress_in_memory)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m._open_fileobj(fileobj, mode, overwrite)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m._open_filelike(fileobj, mode, overwrite)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/astropy/io/fits/file.py:651\u001b[39m, in \u001b[36m_File._open_filename\u001b[39m\u001b[34m(self, filename, mode, overwrite)\u001b[39m\n\u001b[32m    648\u001b[39m ext = os.path.splitext(\u001b[38;5;28mself\u001b[39m.name)[\u001b[32m1\u001b[39m]\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._try_read_compressed(\u001b[38;5;28mself\u001b[39m.name, magic, mode, ext=ext):\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28mself\u001b[39m._file = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIO_FITS_MODES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m     \u001b[38;5;28mself\u001b[39m.close_on_error = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;66;03m# Make certain we're back at the beginning of the file\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;66;03m# BZ2File does not support seek when the file is open for writing, but\u001b[39;00m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# when opening a file for write, bz2.BZ2File always truncates anyway.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'twin_science_reduced.fits'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import annotations\n",
    "\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.coordinates import get_body\n",
    "from astropy.visualization import ImageNormalize, LinearStretch, ZScaleInterval\n",
    "import glob\n",
    "from astropy.stats import sigma_clip\n",
    "import os\n",
    "import pathlib\n",
    "from photutils.datasets import load_star_image\n",
    "import seaborn as sns\n",
    "from astropy.table import Table\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "from photutils.aperture import ApertureStats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import DAOStarFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "image_data = fits.getdata('twin_science_reduced.fits')\n",
    "\n",
    "table = Table(rows)  # contains xcenter, ycenter, flux_r3, flux_r5, ...\n",
    "\n",
    "# Set up figure with background science image\n",
    "plt.imshow(image_data, cmap='inferno', origin='lower', aspect='auto')\n",
    "plt.colorbar(label='Pixel value')\n",
    "\n",
    "# Plot star positions from photometry\n",
    "x = table['xcenter']\n",
    "y = table['ycenter']\n",
    "flux = table['flux_r5']  # or whatever aperture radius you want\n",
    "\n",
    "# Overlay star positions\n",
    "plt.scatter(x, y, s=100, facecolors='none', edgecolors='cyan', label='Stars')\n",
    "\n",
    "# Optionally annotate each with flux value\n",
    "for i in range(len(x)):\n",
    "    plt.text(x[i] + 5, y[i] + 5, f\"{flux[i]:.1f}\", color='white', fontsize=8)\n",
    "\n",
    "# Add labels and save\n",
    "plt.title(\"Photometry on Science Image\")\n",
    "plt.xlabel(\"X Pixel\")\n",
    "plt.ylabel(\"Y Pixel\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "output_path = Path(\"Twin Images\") / 'twin_photometry_overlay.png'\n",
    "plt.savefig(output_path)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved:\", output_path.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22917b2-188c-458e-bae9-8bc11f96cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In flats change dark_filename=None\n",
    "\n",
    "*.fits\n",
    "*.png\n",
    "#if __name__ == \"__main__\":\n",
    " #   median_bias = create_median_bias()\n",
    "  #  print(\"Bias frame created.\")\n",
    "   # print(\"Shape:\", median_bias.shape)\n",
    "    #print(\"Mean:\", np.mean(median_bias))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    median_flat = create_median_flat()\n",
    "    print(\"flatframe created.\")\n",
    "    print(\"Shape:\", median_flat.shape)\n",
    "    print(\"Mean:\", np.mean(median_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70d6d2-a66d-47e2-a0dc-cb6bc320854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_median_bias():\n",
    "    input_dir = Path(\"Data\")\n",
    "    output_file = \"twin_master_bias.fits\"\n",
    "\n",
    "    bias_data = []\n",
    "\n",
    "    for file in sorted(input_dir.glob(\"Bias*.fits\")):\n",
    "        data = fits.getdata(file).astype('f4')\n",
    "\n",
    "        bias_data.append(data) \n",
    "    if len(bias_data) == 0:\n",
    "        raise RuntimeError(f\"No bias\")\n",
    "\n",
    "    bias_3d = np.array(bias_data)\n",
    "\n",
    "    clipping = sigma_clip(bias_3d, cenfunc='median', sigma=3, axis=0)\n",
    "      \n",
    "    median_bias = np.ma.median(clipping, axis=0)\n",
    "\n",
    "    data_for_plot = median_bias.filled(np.nan)\n",
    "    vmin = np.nanpercentile(data_for_plot, 5)\n",
    "    vmax = np.nanpercentile(data_for_plot, 95)\n",
    "\n",
    "    plt.imshow(data_for_plot, origin='lower', cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.savefig(Path(\"Twin_Bias.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save FITS file\n",
    "    output_path = Path(\"Twin Images\") / output_file\n",
    "    primary = fits.PrimaryHDU(data=data_for_plot, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(output_path, overwrite=True)\n",
    "\n",
    "    return data_for_plot\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    " #   median_bias = create_median_bias()\n",
    "  #  print(\"Bias frame created.\")\n",
    "   # print(\"Shape:\", median_bias.shape)\n",
    "    #print(\"Mean:\", np.mean(median_bias))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50065da-1c22-44a8-ac8b-6b7aa2d5cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.coordinates import get_body\n",
    "from astropy.visualization import ImageNormalize, LinearStretch, ZScaleInterval\n",
    "import glob\n",
    "from astropy.stats import sigma_clip\n",
    "import os\n",
    "import pathlib\n",
    "import pytest\n",
    "from photutils.datasets import load_star_image\n",
    "\n",
    "from astroscrappy import detect_cosmics\n",
    "\n",
    "from astropy.table import Table\n",
    "from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
    "from photutils.aperture import ApertureStats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.detection import DAOStarFinder\n",
    "\n",
    "def create_median_bias():\n",
    "    input_dir = Path(\"Data\")\n",
    "    output_file = \"twin_master_bias.fits\"\n",
    "\n",
    "    bias_data = []\n",
    "\n",
    "    for file in sorted(input_dir.glob(\"Bias*.fits\")):\n",
    "        data = fits.getdata(file).astype('f4')\n",
    "\n",
    "        bias_data.append(data) \n",
    "    if len(bias_data) == 0:\n",
    "        raise RuntimeError(f\"No bias\")\n",
    "\n",
    "    bias_3d = np.array(bias_data)\n",
    "\n",
    "    clipping = sigma_clip(bias_3d, cenfunc='median', sigma=3, axis=0)\n",
    "      \n",
    "    median_bias = np.ma.median(clipping, axis=0)\n",
    "\n",
    "    data_for_plot = median_bias.filled(np.nan)\n",
    "    vmin = np.nanpercentile(data_for_plot, 5)\n",
    "    vmax = np.nanpercentile(data_for_plot, 95)\n",
    "\n",
    "    plt.imshow(data_for_plot, origin='lower', cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.savefig(Path(\"Twin_Bias.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    output_path = Path(\"Twin Images\") / output_file\n",
    "    primary = fits.PrimaryHDU(data=data_for_plot, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(output_path, overwrite=True)\n",
    "\n",
    "    return data_for_plot\n",
    "\n",
    "def create_median_dark():\n",
    "    bias_filename = Path(\"Twin Images\") / \"twin_master_bias.fits\"\n",
    "    output_file = Path('twin_master_dark.fits')\n",
    "    bias_frame = fits.getdata(bias_filename).astype('f4')\n",
    "    input_dir = Path(\"Data\")\n",
    "\n",
    "    dark_bias_data = []\n",
    "    \n",
    "    for f in sorted(input_dir.glob(\"Dark*.fits\")):\n",
    "        with fits.open(f) as dark:\n",
    "            dark_data = dark[0].data.astype('f4')\n",
    "            exptime = dark[0].header['EXPTIME']        \n",
    "            dark_NObias = dark_data - bias_frame\n",
    "            dark_bias_data.append(dark_NObias / exptime)\n",
    "            header = dark[0].header.copy()\n",
    "        \n",
    "    one_dark = np.array(dark_bias_data)\n",
    "    clipping_dark = sigma_clip(one_dark, cenfunc='median', sigma=3, axis=0)\n",
    "    median_dark = np.ma.median(clipping_dark, axis=0)\n",
    "    \n",
    "    data_dark = median_dark.filled(np.nan)\n",
    "    vmin = np.nanpercentile(data_dark, 5)\n",
    "    vmax = np.nanpercentile(data_dark, 95)\n",
    "\n",
    "    plt.imshow(data_dark, origin='lower', cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.savefig(Path(\"Twin_Darks.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    output_path = Path(\"Twin Images\") / output_file\n",
    "    primary = fits.PrimaryHDU(data=data_dark, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(output_path, overwrite=True)\n",
    "\n",
    "    return data_dark\n",
    "\n",
    "    \n",
    "def create_median_flat():\n",
    "    bias_filename = Path(\"Twin Images\") / \"twin_master_bias.fits\"\n",
    "    dark_filename = Path(\"Twin Images\") / \"twin_master_dark.fits\"\n",
    "    \n",
    "    output_file = Path(\"Twin Images\") / 'twin_master_flat.fits'\n",
    "    input_dir = Path(\"Data\")\n",
    "\n",
    "    bias_frame = fits.getdata(bias_filename).astype('f4')\n",
    "    dark_frame = fits.getdata(dark_filename).astype('f4')\n",
    "\n",
    "    flat_data = []\n",
    "\n",
    "    for fl in sorted(input_dir.glob('domeflat_*.fits')):\n",
    "        with fits.open(fl) as flat:\n",
    "            data = flat[0].data.astype('f4')\n",
    "            sub_bias = data - bias_frame\n",
    "            flat_data.append(sub_bias)\n",
    "\n",
    "    array = np.array(flat_data)\n",
    "    clipping = sigma_clip(array, cenfunc='median', sigma=3, axis=0)\n",
    "    me_flat = np.ma.median(clipping, axis=0)\n",
    "    median_flat = me_flat / np.ma.median(me_flat)\n",
    "    median_flat = median_flat.filled(np.nan)\n",
    "\n",
    "\n",
    "    vmin, vmax = np.percentile(median_flat, [1, 99])\n",
    "    plt.imshow(median_flat, origin='lower', cmap='gray', vmin=vmin, vmax=vmax)\n",
    "    plt.colorbar(label='Counts')\n",
    "    plt.savefig(Path(\"Twin Images\") / \"Twin_Flats.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    primary = fits.PrimaryHDU(data=median_flat, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(output_file, overwrite=True)\n",
    "\n",
    "    return median_flat\n",
    "\n",
    "\n",
    "\n",
    "def do_aperture_photometry(image, positions, radii, sky_radius_in, sky_annulus_width):\n",
    "\n",
    "    #Opening the data and turning it into flaot 32\n",
    "    with fits.open(image) as im:\n",
    "        image_data = im[0].data.astype('f4')\n",
    "\n",
    "    #Creating an empty list to store data in\n",
    "    rows = []\n",
    "\n",
    "    #Doing a loop over the star\n",
    "    for p in positions:\n",
    "        #Creating dictionary with stars position\n",
    "        row = {'xcenter': p[0], 'ycenter': p[1]}\n",
    "\n",
    "        #Looping over the radii for each star\n",
    "        for r in radii:\n",
    "            \n",
    "            # Doing the aperture and annulus stuff for one star\n",
    "            aperture = CircularAperture(p, r=r)\n",
    "            annulus = CircularAnnulus(p, r_in=sky_radius_in, r_out=sky_radius_in + sky_annulus_width)\n",
    "\n",
    "            # Calculating the aperture and annulus photometry\n",
    "            phot_ap = aperture_photometry(image_data, aperture)\n",
    "            stats = ApertureStats(image_data, annulus, sigma_clip=None)\n",
    "\n",
    "            # finding area of circle and subtracting the sky background to find total flux\n",
    "            aperture_area = aperture.area\n",
    "            background = stats.median * aperture_area\n",
    "            flux = phot_ap['aperture_sum'][0] - background\n",
    "\n",
    "            #Storing results in dictionary with star position and flux \n",
    "            row[f'flux_r{int(r)}'] = flux\n",
    "\n",
    "        #Putting the data into emtpy list\n",
    "        rows.append(row)\n",
    "\n",
    "    # Creating astropy table from dictionary data and adding radii/sky as metadata stuff\n",
    "    table = Table(rows)\n",
    "    table.meta['radii'] = radii\n",
    "    table.meta['sky_radius'] = sky_radius_in\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "def plot_radial_profile(aperture_photometry_data, output_filename=\"radial_profile.png\"):\n",
    "\n",
    "    #Calling radii from aperture data as a array        \n",
    "    radii = np.array(aperture_photometry_data.meta['radii'], dtype = float)\n",
    "\n",
    "    #Getting the sky radius, also from aperture data\n",
    "    sky_radius = aperture_photometry_data.meta['sky_radius']\n",
    "\n",
    "    #taking first flux data for each r \n",
    "    fluxes = [aperture_photometry_data[f'flux_r{int(r)}'][0] for r in radii]\n",
    "\n",
    "    #Plotting radii and fluxes\n",
    "    plt.figure()\n",
    "    plt.plot(radii, fluxes, marker='o', label='Target')\n",
    "\n",
    "    #Sky radius position on graph \n",
    "    plt.axvline(sky_radius, color='gray', linestyle='--', label='Sky radius')\n",
    "\n",
    "    #Making it look nice\n",
    "    plt.xlabel('Radius')\n",
    "    plt.ylabel('Flux')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #Saving everything\n",
    "    plt.savefig(output_filename)\n",
    "    plt.close()\n",
    "\n",
    "def calculate_gain(files):\n",
    "\n",
    "    #Creating empty list to store data in\n",
    "    flats = []\n",
    "\n",
    "    #Opening files data\n",
    "    for f in files:\n",
    "        with fits.open(f) as file: \n",
    "\n",
    "            #Trimming data\n",
    "            trim = file[0].data[1600:2000, 1300:1700]\n",
    "\n",
    "            #Putting it into empty list\n",
    "            flats.append(trim)\n",
    "\n",
    "    #Unpacking flats\n",
    "    flats1, flats2 = flats\n",
    "\n",
    "    #Getting the difference of the two flats\n",
    "    flat_diff = flats1 - flats2\n",
    "\n",
    "    #Calculating variance \n",
    "    flat_var = np.var(flat_diff)\n",
    "\n",
    "    #Getting average between the two\n",
    "    mean = 0.5 * np.mean(flats1 + flats2)\n",
    "\n",
    "    #Getting the gain with formula \n",
    "    gain = 2 * mean / flat_var\n",
    "\n",
    "    return gain\n",
    "\n",
    "\n",
    "def calculate_readout_noise(files, gain):\n",
    "\n",
    "    #Creating another empty list\n",
    "    file_data = []\n",
    "\n",
    "    #Opening files and trimming then putting into list\n",
    "    for f in files:\n",
    "        with fits.open(f) as file: \n",
    "            trim = file[0].data[1000:-1000, 1000:-1000]\n",
    "            file_data.append(trim)\n",
    "\n",
    "    #Unpacking files\n",
    "    bias1, bias2 = file_data\n",
    "            \n",
    "    # Calculate the variance of the difference between the two images\n",
    "    bias_diff = bias1 - bias2\n",
    "    bias_diff_var = np.var(bias_diff)\n",
    "\n",
    "    # Calculate the readout noise\n",
    "    readout_noise_adu = np.sqrt(bias_diff_var / 2)\n",
    "    readout_noise_e = readout_noise_adu * gain\n",
    "\n",
    "    return readout_noise_e\n",
    "\n",
    "\n",
    "def run_reduction(data_dir):\n",
    "    \n",
    "    data = Path(data_dir)\n",
    "\n",
    "    #Getting bias data\n",
    "    median_bias_path = data / \"median_bias.fits\"\n",
    "    bias_files = list(data.glob(\"Bias*.fit\"))\n",
    "    create_median_bias(bias_files, median_bias_path)\n",
    "    median_bias = fits.getdata(median_bias_path).astype('f4')\n",
    "\n",
    "    #Getting dark data\n",
    "    median_dark_path = data / \"median_dark.fits\"\n",
    "    dark_files = list(data.glob(\"Dark*.fit\"))\n",
    "    create_median_dark(dark_files, median_bias_path, median_dark_path)\n",
    "    median_dark = fits.getdata(median_dark_path).astype('f4')\n",
    "\n",
    "    #Getting flat data\n",
    "    median_flat_path = data / \"median_flat.fits\"\n",
    "    flat_files = list(data.glob(\"AutoFlat*.fit\"))\n",
    "    create_median_flat(flat_files, median_bias_path, median_flat_path, dark_filename=median_dark_path)\n",
    "    median_flat = fits.getdata(median_flat_path).astype('f4')\n",
    "\n",
    "    #getting science data\n",
    "    science_files = sorted(data.glob(\"kelt-16-b-S001-R001-C*-r.fit\"))\n",
    "\n",
    "    #Creating empty list to put stuff in later\n",
    "    science = []\n",
    "\n",
    "    #Opening science files\n",
    "    for s in science_files:\n",
    "        \n",
    "       #Making a path so it works correctly\n",
    "        output_filename = data / (s.stem + \"_reduced.fits\")\n",
    "\n",
    "\n",
    "        #Doing the actual reductions of image, something wrong here \n",
    "        reduced_path = reduce_science_frame(s, median_bias_path, median_dark_path, median_flat_path,reduced_science_filename = output_filename)\n",
    "\n",
    "        \n",
    "        print(\"Type of reduced_path:\", type(reduced_path))\n",
    "\n",
    "        #putting it all back into llist\n",
    "        science.append(str(reduced_path))\n",
    "\n",
    "    #Trying to open it as an image, I am not sure it is actually passing as an image\n",
    "    if science:\n",
    "        image = science[0]\n",
    "        with fits.open(image) as hdul:\n",
    "            image_data = hdul[0].data.astype('f4')\n",
    "\n",
    "        # Doing the stuff i did in one of the functions and pulling from prof examples\n",
    "        #Getting stars\n",
    "        mean, median, std = sigma_clipped_stats(image_data, sigma=3.0)\n",
    "        daofind = DAOStarFinder(fwhm=3.0, threshold=5.*std)\n",
    "        sources = daofind(image_data - median)\n",
    "\n",
    "        #Getting positions\n",
    "        positions = [(src['xcentroid'], src['ycentroid']) for src in sources]\n",
    "\n",
    "        # giving it data, idk how to pull this like i did in the functions, I am so tired\n",
    "        aperture_radius = 4\n",
    "        sky_radius_in = 6\n",
    "        sky_annulus_width = 2\n",
    "\n",
    "        #This will not work, I am passing something as a string when I am not suppposed to:(\n",
    "        #Trying to run photometry and save it\n",
    "        phot_table = do_aperture_photometry(image_data, positions, aperture_radius, sky_radius_in, sky_annulus_width, plot_path=data / \"photometry_plot.png\")\n",
    "\n",
    "        print(phot_table)\n",
    "\n",
    "        #Trying to save it\n",
    "        phot_table.write(data /\"photometry_results.csv\", format=\"csv\", overwrite=True)\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def reduce_science_frame(science_filename, median_bias_filename, median_flat_filename, median_dark_filename, reduced_science_filename=\"reduced_science.fits\"):\n",
    "\n",
    "    #Opening each file and turning it into float 32\n",
    "    bias_frame = fits.getdata(median_bias_filename).astype('f4')\n",
    "    flat_frame = fits.getdata(median_flat_filename).astype('f4')\n",
    "    dark_frame = fits.getdata(median_dark_filename).astype('f4')\n",
    "\n",
    "    #Opening science stuff and trimming/float 32 \n",
    "    with fits.open(science_filename) as science:\n",
    "        data_s = science[0].data.astype('f4')[1000:1500, 2000:2500]\n",
    "\n",
    "        #Getting the exposure time from header\n",
    "        header = science[0].header\n",
    "        exptime = science[0].header['EXPTIME']\n",
    "  \n",
    "    #Removing bias signal\n",
    "    sub_bias = data_s - bias_frame\n",
    "\n",
    "    #Subtracting dark frame with corrected exptime\n",
    "    dark_corrected = sub_bias - dark_frame * exptime\n",
    "\n",
    "    #Normalizing the flat\n",
    "    flat_norm = flat_frame / np.mean(flat_frame)\n",
    "\n",
    "    #Normalzing even more \n",
    "    corrected_science = dark_corrected / flat_norm\n",
    "    \n",
    "    #Getting rid of cosmic rays\n",
    "    mask, cleaned = detect_cosmics(corrected_science)\n",
    "    reduced_science = cleaned\n",
    "\n",
    "    #Saving\n",
    "    hdu = fits.PrimaryHDU(data=reduced_science)\n",
    "    hdu.writeto(str(reduced_science_filename), overwrite = True)\n",
    "\n",
    "    return reduced_science\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154836eb-e8a3-4051-ab06-793d6f16fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OlD CODE CCD \n",
    "\n",
    "\n",
    "def create_median_bias(bias_list, median_bias_filename):\n",
    "\n",
    "    #creating a list to put bias in\n",
    "    bias_data = []\n",
    "\n",
    "    #Calling the files in bias list with a loop\n",
    "    for file in bias_list:\n",
    "\n",
    "        #Extracting data and turning it into float 32, also trimming it so that the kernal doesnt crash\n",
    "        data = fits.getdata(file)[1000:1500, 2000:2500].astype('f4')\n",
    "\n",
    "        #Appending it to the list I created \n",
    "        bias_data.append(data) \n",
    "\n",
    "    #Turning it into a 3d array\n",
    "    bias_3d = np.array(bias_data)\n",
    "\n",
    "    #Clipping all the data outside 2.5, collasping along y-axis, and returning a masked array\n",
    "    clipping = sigma_clip(bias_3d, cenfunc='median', sigma=3, axis=0)\n",
    "      \n",
    "    #Collasping along y-axis and getting a 2d array\n",
    "    median_bias = np.ma.median(clipping, axis=0)\n",
    "\n",
    "\n",
    "    # Saving it to median_bias_filename \n",
    "    primary = fits.PrimaryHDU(data=median_bias.data, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_bias_filename, overwrite=True)\n",
    "    \n",
    "    return median_bias.data\n",
    "\n",
    "def create_median_dark(dark_list, bias_filename, median_dark_filename):\n",
    "    bias_frame = fits.getdata(bias_filename).astype('f4')\n",
    "\n",
    "    #Creating an empty list to store dark frames\n",
    "    dark_bias_data = []\n",
    "    \n",
    "    #Calling the data from dark_list\n",
    "    for f in dark_list:\n",
    "        with fits.open(f) as dark:\n",
    "        \n",
    "        #getting raw data and changing into float 32, then trimming so it doesnt crash\n",
    "            dark_data = dark[0].data.astype('f4')[1000:1500, 2000:2500]\n",
    "        \n",
    "        #Getting the exposure time\n",
    "            exptime = dark[0].header['EXPTIME']\n",
    "        \n",
    "        #SUbtracting the bias frame from the dark frame (bias was trimmed to same numbers in other one)\n",
    "            dark_NObias = dark_data - bias_frame\n",
    "        \n",
    "        #divinding the bias by exp time, becomes 2d array and is appended into dark_bias_data list\n",
    "            dark_bias_data.append(dark_NObias / exptime)\n",
    "\n",
    "            header = dark[0].header.copy()\n",
    "\n",
    "        \n",
    "    one_dark = np.array(dark_bias_data)\n",
    "    #Combining the dark frames and removing cosmic rays/hot pixels\n",
    "    clipping_dark = sigma_clip(one_dark, cenfunc='median', sigma=3, axis=0)\n",
    "\n",
    "    #Finding the median across all the dark frames and collasping along one axis to get a 2d array\n",
    "    median_dark = np.ma.median(clipping_dark, axis=0)\n",
    "\n",
    "\n",
    "    #Creating another fits file \n",
    "    dark_hdu = fits.PrimaryHDU(data=median_dark.data, header=dark[0].header)\n",
    "    #Adding comments to the header\n",
    "    dark_hdu.header['COMMENT'] = 'Combined dark image with bias subtracted'\n",
    "    dark_hdu.header['BIASFILE'] = (str(bias_filename), 'Bias image used to subtract bias level')\n",
    "\n",
    "    #Saving the dark image \n",
    "    hdu = fits.PrimaryHDU(data=median_dark.data, header=fits.Header())\n",
    "    dark_hdu.writeto(median_dark_filename, overwrite=True)\n",
    "\n",
    "    return median_dark.data\n",
    "\n",
    "\n",
    "def create_median_flat(flat_list, bias_filename, median_flat_filename, dark_filename):\n",
    "\n",
    "    #Getting data\n",
    "    bias_frame = fits.getdata(bias_filename).astype('f4')\n",
    "\n",
    "    #Creating an empty list\n",
    "    flat_data = []\n",
    "\n",
    "    #Opening flat list\n",
    "    for fl in flat_list:\n",
    "        with fits.open(fl) as flat:\n",
    "            #turning the data into a float 32, then trimming it\n",
    "            data = flat[0].data.astype('f4')[1000:1500, 2000:2500]\n",
    "\n",
    "            #Subtracting the data by the bias\n",
    "            sub_bias = data - bias_frame\n",
    "\n",
    "            #Putting it back in my list\n",
    "            flat_data.append(sub_bias)\n",
    "\n",
    "    #Turning into array\n",
    "    array = np.array(flat_data)\n",
    "\n",
    "    #clipping it with sigma 3\n",
    "    clipping = sigma_clip(array, cenfunc='median', sigma=3, axis=0)\n",
    "\n",
    "    #Taking median\n",
    "    me_flat = np.ma.median(clipping, axis=0)\n",
    "\n",
    "    #Normalizing the flats\n",
    "    median_flat = me_flat / np.ma.median(me_flat)\n",
    "\n",
    "    #Saving everything\n",
    "    hdu = fits.PrimaryHDU(data=median_flat.data)\n",
    "    hdu.header['COMMENT'] = 'Normalized median flat without bias'\n",
    "    hdu.writeto(median_flat_filename, overwrite = True)\n",
    "\n",
    "    return median_flat.data\n",
    "\n",
    "\n",
    "def plot_flat(median_flat_filename, ouput_filename=\"median_flat.png\", profile_ouput_filename=\"median_flat_profile.png\"):\n",
    "\n",
    "    #Getting the data and turning it into a float 32\n",
    "    flat_file = fits.getdata(median_flat_filename).astype('f4')\n",
    "    \n",
    "    #Plotting the flat frame\n",
    "    plt.imshow(flat_file, vmin=0.9, vmax=1.1)\n",
    "\n",
    "    #Saving it\n",
    "    plt.savefig(ouput_filename)\n",
    "    plt.close()\n",
    "\n",
    "    #Turning it into a 1D array\n",
    "    median = np.ma.median(flat_file, axis=0)\n",
    "\n",
    "    #Plotting the 1D array \n",
    "    plt.figure()\n",
    "    plt.plot(median)\n",
    "    plt.grid(True)\n",
    "\n",
    "    #Saving it \n",
    "    plt.savefig(profile_ouput_filename)\n",
    "    plt.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def do_aperture_photometry(image, positions, radii, sky_radius_in, sky_annulus_width):\n",
    "\n",
    "    #Opening the data and turning it into flaot 32\n",
    "    with fits.open(image) as im:\n",
    "        image_data = im[0].data.astype('f4')\n",
    "\n",
    "    #Creating an empty list to store data in\n",
    "    rows = []\n",
    "\n",
    "    #Doing a loop over the star\n",
    "    for p in positions:\n",
    "        #Creating dictionary with stars position\n",
    "        row = {'xcenter': p[0], 'ycenter': p[1]}\n",
    "\n",
    "        #Looping over the radii for each star\n",
    "        for r in radii:\n",
    "            \n",
    "            # Doing the aperture and annulus stuff for one star\n",
    "            aperture = CircularAperture(p, r=r)\n",
    "            annulus = CircularAnnulus(p, r_in=sky_radius_in, r_out=sky_radius_in + sky_annulus_width)\n",
    "\n",
    "            # Calculating the aperture and annulus photometry\n",
    "            phot_ap = aperture_photometry(image_data, aperture)\n",
    "            stats = ApertureStats(image_data, annulus, sigma_clip=None)\n",
    "\n",
    "            # finding area of circle and subtracting the sky background to find total flux\n",
    "            aperture_area = aperture.area\n",
    "            background = stats.median * aperture_area\n",
    "            flux = phot_ap['aperture_sum'][0] - background\n",
    "\n",
    "            #Storing results in dictionary with star position and flux \n",
    "            row[f'flux_r{int(r)}'] = flux\n",
    "\n",
    "        #Putting the data into emtpy list\n",
    "        rows.append(row)\n",
    "\n",
    "    # Creating astropy table from dictionary data and adding radii/sky as metadata stuff\n",
    "    table = Table(rows)\n",
    "    table.meta['radii'] = radii\n",
    "    table.meta['sky_radius'] = sky_radius_in\n",
    "    \n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "def plot_radial_profile(aperture_photometry_data, output_filename=\"radial_profile.png\"):\n",
    "\n",
    "    #Calling radii from aperture data as a array        \n",
    "    radii = np.array(aperture_photometry_data.meta['radii'], dtype = float)\n",
    "\n",
    "    #Getting the sky radius, also from aperture data\n",
    "    sky_radius = aperture_photometry_data.meta['sky_radius']\n",
    "\n",
    "    #taking first flux data for each r \n",
    "    fluxes = [aperture_photometry_data[f'flux_r{int(r)}'][0] for r in radii]\n",
    "\n",
    "    #Plotting radii and fluxes\n",
    "    plt.figure()\n",
    "    plt.plot(radii, fluxes, marker='o', label='Target')\n",
    "\n",
    "    #Sky radius position on graph \n",
    "    plt.axvline(sky_radius, color='gray', linestyle='--', label='Sky radius')\n",
    "\n",
    "    #Making it look nice\n",
    "    plt.xlabel('Radius')\n",
    "    plt.ylabel('Flux')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #Saving everything\n",
    "    plt.savefig(output_filename)\n",
    "    plt.close()\n",
    "\n",
    "def calculate_gain(files):\n",
    "\n",
    "    #Creating empty list to store data in\n",
    "    flats = []\n",
    "\n",
    "    #Opening files data\n",
    "    for f in files:\n",
    "        with fits.open(f) as file: \n",
    "\n",
    "            #Trimming data\n",
    "            trim = file[0].data[1600:2000, 1300:1700]\n",
    "\n",
    "            #Putting it into empty list\n",
    "            flats.append(trim)\n",
    "\n",
    "    #Unpacking flats\n",
    "    flats1, flats2 = flats\n",
    "\n",
    "    #Getting the difference of the two flats\n",
    "    flat_diff = flats1 - flats2\n",
    "\n",
    "    #Calculating variance \n",
    "    flat_var = np.var(flat_diff)\n",
    "\n",
    "    #Getting average between the two\n",
    "    mean = 0.5 * np.mean(flats1 + flats2)\n",
    "\n",
    "    #Getting the gain with formula \n",
    "    gain = 2 * mean / flat_var\n",
    "\n",
    "    return gain\n",
    "\n",
    "\n",
    "def calculate_readout_noise(files, gain):\n",
    "\n",
    "    #Creating another empty list\n",
    "    file_data = []\n",
    "\n",
    "    #Opening files and trimming then putting into list\n",
    "    for f in files:\n",
    "        with fits.open(f) as file: \n",
    "            trim = file[0].data[1000:-1000, 1000:-1000]\n",
    "            file_data.append(trim)\n",
    "\n",
    "    #Unpacking files\n",
    "    bias1, bias2 = file_data\n",
    "            \n",
    "    # Calculate the variance of the difference between the two images\n",
    "    bias_diff = bias1 - bias2\n",
    "    bias_diff_var = np.var(bias_diff)\n",
    "\n",
    "    # Calculate the readout noise\n",
    "    readout_noise_adu = np.sqrt(bias_diff_var / 2)\n",
    "    readout_noise_e = readout_noise_adu * gain\n",
    "\n",
    "    return readout_noise_e\n",
    "\n",
    "\n",
    "def run_reduction(data_dir):\n",
    "    \n",
    "    data = Path(data_dir)\n",
    "\n",
    "    #Getting bias data\n",
    "    median_bias_path = data / \"median_bias.fits\"\n",
    "    bias_files = list(data.glob(\"Bias*.fit\"))\n",
    "    create_median_bias(bias_files, median_bias_path)\n",
    "    median_bias = fits.getdata(median_bias_path).astype('f4')\n",
    "\n",
    "    #Getting dark data\n",
    "    median_dark_path = data / \"median_dark.fits\"\n",
    "    dark_files = list(data.glob(\"Dark*.fit\"))\n",
    "    create_median_dark(dark_files, median_bias_path, median_dark_path)\n",
    "    median_dark = fits.getdata(median_dark_path).astype('f4')\n",
    "\n",
    "    #Getting flat data\n",
    "    median_flat_path = data / \"median_flat.fits\"\n",
    "    flat_files = list(data.glob(\"AutoFlat*.fit\"))\n",
    "    create_median_flat(flat_files, median_bias_path, median_flat_path, dark_filename=median_dark_path)\n",
    "    median_flat = fits.getdata(median_flat_path).astype('f4')\n",
    "\n",
    "    #getting science data\n",
    "    science_files = sorted(data.glob(\"kelt-16-b-S001-R001-C*-r.fit\"))\n",
    "\n",
    "    #Creating empty list to put stuff in later\n",
    "    science = []\n",
    "\n",
    "    #Opening science files\n",
    "    for s in science_files:\n",
    "        \n",
    "       #Making a path so it works correctly\n",
    "        output_filename = data / (s.stem + \"_reduced.fits\")\n",
    "\n",
    "\n",
    "        #Doing the actual reductions of image, something wrong here \n",
    "        reduced_path = reduce_science_frame(s, median_bias_path, median_dark_path, median_flat_path,reduced_science_filename = output_filename)\n",
    "\n",
    "        \n",
    "        print(\"Type of reduced_path:\", type(reduced_path))\n",
    "\n",
    "        #putting it all back into llist\n",
    "        science.append(str(reduced_path))\n",
    "\n",
    "    #Trying to open it as an image, I am not sure it is actually passing as an image\n",
    "    if science:\n",
    "        image = science[0]\n",
    "        with fits.open(image) as hdul:\n",
    "            image_data = hdul[0].data.astype('f4')\n",
    "\n",
    "        # Doing the stuff i did in one of the functions and pulling from prof examples\n",
    "        #Getting stars\n",
    "        mean, median, std = sigma_clipped_stats(image_data, sigma=3.0)\n",
    "        daofind = DAOStarFinder(fwhm=3.0, threshold=5.*std)\n",
    "        sources = daofind(image_data - median)\n",
    "\n",
    "        #Getting positions\n",
    "        positions = [(src['xcentroid'], src['ycentroid']) for src in sources]\n",
    "\n",
    "        # giving it data, idk how to pull this like i did in the functions, I am so tired\n",
    "        aperture_radius = 4\n",
    "        sky_radius_in = 6\n",
    "        sky_annulus_width = 2\n",
    "\n",
    "        #This will not work, I am passing something as a string when I am not suppposed to:(\n",
    "        #Trying to run photometry and save it\n",
    "        phot_table = do_aperture_photometry(image_data, positions, aperture_radius, sky_radius_in, sky_annulus_width, plot_path=data / \"photometry_plot.png\")\n",
    "\n",
    "        print(phot_table)\n",
    "\n",
    "        #Trying to save it\n",
    "        phot_table.write(data /\"photometry_results.csv\", format=\"csv\", overwrite=True)\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "def reduce_science_frame(science_filename, median_bias_filename, median_flat_filename, median_dark_filename, reduced_science_filename=\"reduced_science.fits\"):\n",
    "\n",
    "    #Opening each file and turning it into float 32\n",
    "    bias_frame = fits.getdata(median_bias_filename).astype('f4')\n",
    "    flat_frame = fits.getdata(median_flat_filename).astype('f4')\n",
    "    dark_frame = fits.getdata(median_dark_filename).astype('f4')\n",
    "\n",
    "    #Opening science stuff and trimming/float 32 \n",
    "    with fits.open(science_filename) as science:\n",
    "        data_s = science[0].data.astype('f4')[1000:1500, 2000:2500]\n",
    "\n",
    "        #Getting the exposure time from header\n",
    "        header = science[0].header\n",
    "        exptime = science[0].header['EXPTIME']\n",
    "  \n",
    "    #Removing bias signal\n",
    "    sub_bias = data_s - bias_frame\n",
    "\n",
    "    #Subtracting dark frame with corrected exptime\n",
    "    dark_corrected = sub_bias - dark_frame * exptime\n",
    "\n",
    "    #Normalizing the flat\n",
    "    flat_norm = flat_frame / np.mean(flat_frame)\n",
    "\n",
    "    #Normalzing even more \n",
    "    corrected_science = dark_corrected / flat_norm\n",
    "    \n",
    "    #Getting rid of cosmic rays\n",
    "    mask, cleaned = detect_cosmics(corrected_science)\n",
    "    reduced_science = cleaned\n",
    "\n",
    "    #Saving\n",
    "    hdu = fits.PrimaryHDU(data=reduced_science)\n",
    "    hdu.writeto(str(reduced_science_filename), overwrite = True)\n",
    "\n",
    "    return reduced_science\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
